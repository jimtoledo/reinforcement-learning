{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Code implementations derived from https://github.com/mimoralea/gdrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCDAP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 output_dim,\n",
    "                 hidden_dims=(32,32), #define hidden layers as tuple where each element is an int representing # of neurons at a layer\n",
    "                 activation_fc=nn.ReLU):\n",
    "        super(FCDAP, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            hidden_layers.append(activation_fc())\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            activation_fc(),\n",
    "            *hidden_layers,\n",
    "            nn.Linear(hidden_dims[-1], output_dim)\n",
    "        )\n",
    "\n",
    "        device = \"cpu\"\n",
    "        #if torch.cuda.is_available():\n",
    "        #    device = \"cuda\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def _format(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = self._format(state)\n",
    "        return self.layers(x)\n",
    "\n",
    "    #select and return action, corresponding log prob of the action, and entropy of the distribution\n",
    "    def select_action(self, state):\n",
    "        logits = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action).unsqueeze(-1), dist.entropy().unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fully-connected value network (state observation -> state value)\n",
    "class FCV(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 hidden_dims=(32,32), #define hidden layers as tuple where each element is an int representing # of neurons at a layer\n",
    "                 activation_fc=nn.ReLU):\n",
    "        super(FCV, self).__init__()\n",
    "\n",
    "        hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            hidden_layers.append(activation_fc())\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            activation_fc(),\n",
    "            *hidden_layers,\n",
    "            nn.Linear(hidden_dims[-1], 1)\n",
    "        )\n",
    "\n",
    "        device = \"cpu\"\n",
    "        #if torch.cuda.is_available():\n",
    "        #    device = \"cuda\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def _format(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self._format(state)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedAdam(torch.optim.Adam):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=False):\n",
    "        super(SharedAdam, self).__init__(\n",
    "            params, lr=lr, betas=betas, eps=eps, \n",
    "            weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = 0\n",
    "                state['shared_step'] = torch.zeros(1).share_memory_()\n",
    "                state['exp_avg'] = torch.zeros_like(p.data).share_memory_()\n",
    "                state['exp_avg_sq'] = torch.zeros_like(p.data).share_memory_()\n",
    "                if weight_decay:\n",
    "                    state['weight_decay'] = torch.zeros_like(p.data).share_memory_()\n",
    "                if amsgrad:\n",
    "                    state['max_exp_avg_sq'] = torch.zeros_like(p.data).share_memory_()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                self.state[p]['steps'] = self.state[p]['shared_step'].item()\n",
    "                self.state[p]['shared_step'] += 1\n",
    "        super().step(closure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image.png)\n",
    "https://arxiv.org/pdf/1602.01783.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "class A3CWorker(mp.Process):\n",
    "    def __init__(self, rank, make_env_fn, shared_policy_model, shared_value_model, shared_policy_optimizer, shared_value_optimizer, shared_T, max_T, max_td_steps=5, gamma=1.0, entropy_weight=1e-4, seed=0):\n",
    "        super(A3CWorker, self).__init__()\n",
    "        self.seed = seed + rank\n",
    "        self.rank = rank\n",
    "        self.env = make_env_fn()\n",
    "        self.gamma = gamma\n",
    "        self.entropy_weight = entropy_weight\n",
    "        self.shared_policy_model = shared_policy_model\n",
    "        self.shared_value_model = shared_value_model\n",
    "        self.shared_policy_optimizer = shared_policy_optimizer\n",
    "        self.shared_value_optimizer = shared_value_optimizer\n",
    "        self.max_td_steps = max_td_steps\n",
    "        self.T = shared_T\n",
    "        self.max_T = max_T\n",
    "    \n",
    "    def _optimize_model(self, rewards, values, log_probs, entropies):\n",
    "        T = len(rewards)\n",
    "        #Calculate n_step returns\n",
    "        discounts = np.logspace(0, T, num=T, base=self.gamma, endpoint=False)\n",
    "        returns = np.array([np.sum(discounts[:T-t] * rewards[t:]) for t in range(T)])\n",
    "        #drop return of final td step next_state\n",
    "        discounts = torch.FloatTensor(discounts[:-1]).unsqueeze(1)\n",
    "        returns = torch.FloatTensor(returns[:-1]).unsqueeze(1)\n",
    "        \n",
    "        log_probs = torch.cat(log_probs)\n",
    "        entropies = torch.cat(entropies)\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        #calculate \"policy loss\" as the negative policy gradient with weighted entropy\n",
    "        advantages = returns - values #use advantage estimates (A_t = G_t-V(S_t)) instead of returns for policy gradient\n",
    "        policy_grad = (advantages.detach() * log_probs).mean()\n",
    "        policy_loss = -(policy_grad + self.entropy_weight*entropies.mean())\n",
    "\n",
    "        self.shared_policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        #transfer gradients from local model to shared model and step\n",
    "        for param, shared_param in zip(self.local_policy_model.parameters(), \n",
    "                                       self.shared_policy_model.parameters()):\n",
    "            shared_param._grad = param.grad\n",
    "        self.shared_policy_optimizer.step()\n",
    "        #load updated shared model back into local model\n",
    "        self.local_policy_model.load_state_dict(self.shared_policy_model.state_dict())\n",
    "\n",
    "        value_loss = advantages.pow(2).mul(0.5).mean() #mean square error\n",
    "        self.shared_value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        #transfer gradients from local model to shared model and step\n",
    "        for param, shared_param in zip(self.local_value_model.parameters(), \n",
    "                                       self.shared_value_model.parameters()):\n",
    "            shared_param._grad = param.grad\n",
    "        self.shared_value_optimizer.step()\n",
    "        #load updated shared model back into local model\n",
    "        self.local_value_model.load_state_dict(self.shared_value_model.state_dict())\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed) #set seeds\n",
    "        #initialize local models by deep-copying shared models\n",
    "        self.local_policy_model = deepcopy(self.shared_policy_model)\n",
    "        self.local_value_model = deepcopy(self.shared_value_model)\n",
    "        state = self.env.reset(seed=self.seed)[0]\n",
    "        while self.T < self.max_T:\n",
    "            log_probs, rewards, values, entropies = [], [], [], []\n",
    "            #gather data for n_step td\n",
    "            for _ in range(self.max_td_steps):\n",
    "                action, log_prob, entropy = self.local_policy_model.select_action(state) #select action and get corresponding log prob and dist entropy\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                #gather log probs, rewards, and entropies to calculate policy gradient\n",
    "                log_probs.append(log_prob)\n",
    "                rewards.append(reward)\n",
    "                entropies.append(entropy)\n",
    "                #get estimated value of current state\n",
    "                values.append(self.local_value_model(state))      \n",
    "                if terminated or truncated:\n",
    "                    state = self.env.reset(seed=self.seed)[0]\n",
    "                else:\n",
    "                    state = next_state\n",
    "                \n",
    "                self.T += 1\n",
    "\n",
    "            #value of next state for final td step\n",
    "            R = 0 if terminated else self.local_value_model(next_state)\n",
    "            rewards.append(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3C():\n",
    "    def __init__(self, \n",
    "                policy_model_fn = lambda num_obs, nA: FCDAP(num_obs, nA), #state vars, nA -> model\n",
    "                policy_optimizer_fn = lambda params, lr : SharedAdam(params, lr), #model params, lr -> optimizer\n",
    "                policy_optimizer_lr = 1e-4, #optimizer learning rate\n",
    "                value_model_fn = lambda num_obs: FCV(num_obs), #state vars  -> model\n",
    "                value_optimizer_fn = lambda params, lr : SharedAdam(params, lr), #model params, lr -> optimizer\n",
    "                value_optimizer_lr = 1e-4, #optimizer learning rate\n",
    "                entropy_weight = 1e-4\n",
    "                ):\n",
    "        self.policy_model_fn = policy_model_fn\n",
    "        self.policy_optimizer_fn = policy_optimizer_fn\n",
    "        self.policy_optimizer_lr = policy_optimizer_lr\n",
    "        self.value_model_fn = value_model_fn\n",
    "        self.value_optimizer_fn = value_optimizer_fn\n",
    "        self.value_optimizer_lr = value_optimizer_lr\n",
    "        self.entropy_weight = entropy_weight\n",
    "\n",
    "    def _init_model(self, env, policy_lr=None, value_lr=None):\n",
    "        if not policy_lr:\n",
    "            policy_lr = self.policy_optimizer_lr\n",
    "        if not value_lr:\n",
    "            value_lr = self.value_optimizer_lr\n",
    "\n",
    "        self.policy_model = self.policy_model_fn(len(env.observation_space.sample()), env.action_space.n)\n",
    "        self.policy_optimizer = self.policy_optimizer_fn(self.policy_model.parameters(), lr=policy_lr)\n",
    "\n",
    "        self.value_model = self.value_model_fn(len(env.observation_space.sample()))\n",
    "        self.value_optimizer = self.value_optimizer_fn(self.value_model.parameters(), lr=value_lr)\n",
    "\n",
    "    def train(self, make_env_fn, gamma=1.0, num_episodes=100, policy_lr=None, value_lr=None, save_models=None):\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
