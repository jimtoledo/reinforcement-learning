{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Code implementations derived from https://github.com/mimoralea/gdrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCDAP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 output_dim,\n",
    "                 hidden_dims=(32,32), #define hidden layers as tuple where each element is an int representing # of neurons at a layer\n",
    "                 activation_fc=nn.ReLU):\n",
    "        super(FCDAP, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            hidden_layers.append(activation_fc())\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            activation_fc(),\n",
    "            *hidden_layers,\n",
    "            nn.Linear(hidden_dims[-1], output_dim)\n",
    "        )\n",
    "\n",
    "        device = \"cpu\"\n",
    "        #if torch.cuda.is_available():\n",
    "        #    device = \"cuda\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def _format(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = self._format(state)\n",
    "        return self.layers(x)\n",
    "\n",
    "    #select and return action, corresponding log prob of the action, and entropy of the distribution\n",
    "    def select_action(self, state):\n",
    "        logits = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action).unsqueeze(-1), dist.entropy().unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fully-connected value network (state observation -> state value)\n",
    "class FCV(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 hidden_dims=(32,32), #define hidden layers as tuple where each element is an int representing # of neurons at a layer\n",
    "                 activation_fc=nn.ReLU):\n",
    "        super(FCV, self).__init__()\n",
    "\n",
    "        hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            hidden_layers.append(activation_fc())\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            activation_fc(),\n",
    "            *hidden_layers,\n",
    "            nn.Linear(hidden_dims[-1], 1)\n",
    "        )\n",
    "\n",
    "        device = \"cpu\"\n",
    "        #if torch.cuda.is_available():\n",
    "        #    device = \"cuda\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def _format(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self._format(state)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPG():\n",
    "    def __init__(self, \n",
    "                policy_model_fn = lambda num_obs, nA: FCDAP(num_obs, nA), #state vars, nA -> model\n",
    "                policy_optimizer_fn = lambda params, lr : optim.Adam(params, lr), #model params, lr -> optimizer\n",
    "                policy_optimizer_lr = 1e-4, #optimizer learning rate\n",
    "                value_model_fn = lambda num_obs: FCV(num_obs), #state vars  -> model\n",
    "                value_optimizer_fn = lambda params, lr : optim.Adam(params, lr), #model params, lr -> optimizer\n",
    "                value_optimizer_lr = 1e-4, #optimizer learning rate\n",
    "                entropy_weight = 1e-4\n",
    "                ):\n",
    "        self.policy_model_fn = policy_model_fn\n",
    "        self.policy_optimizer_fn = policy_optimizer_fn\n",
    "        self.policy_optimizer_lr = policy_optimizer_lr\n",
    "        self.value_model_fn = value_model_fn\n",
    "        self.value_optimizer_fn = value_optimizer_fn\n",
    "        self.value_optimizer_lr = value_optimizer_lr\n",
    "        self.entropy_weight = entropy_weight\n",
    "\n",
    "    def _init_model(self, env, policy_lr=None, value_lr=None):\n",
    "        if not policy_lr:\n",
    "            policy_lr = self.policy_optimizer_lr\n",
    "        if not value_lr:\n",
    "            value_lr = self.value_optimizer_lr\n",
    "\n",
    "        self.policy_model = self.policy_model_fn(len(env.observation_space.sample()), env.action_space.n)\n",
    "        self.policy_optimizer = self.policy_optimizer_fn(self.policy_model.parameters(), lr=policy_lr)\n",
    "\n",
    "        self.value_model = self.value_model_fn(len(env.observation_space.sample()))\n",
    "        self.value_optimizer = self.value_optimizer_fn(self.value_model.parameters(), lr=value_lr)\n",
    "    \n",
    "    def _optimize_model(self, rewards, values, log_probs, entropies):\n",
    "        T = len(rewards)\n",
    "        #calculate returns G_t(tau)\n",
    "        discounts = np.logspace(0, T, num=T, base=self.gamma, endpoint=False)\n",
    "        returns = torch.FloatTensor([np.sum(discounts[:T-t] * rewards[t:]) for t in range(T)]).unsqueeze(1)\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        entropies = torch.cat(entropies)\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        #calculate \"policy loss\" as the negative policy gradient with weighted entropy\n",
    "        advantages = returns - values #use advantage estimates (A_t = G_t-V(S_t)) instead of returns for policy gradient\n",
    "        policy_grad = (advantages.detach() * log_probs).mean()\n",
    "        policy_loss = -(policy_grad + self.entropy_weight*entropies.mean())\n",
    "\n",
    "        #optimize policy network (gradient descent)\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "        #optimize value network\n",
    "        value_loss = advantages.pow(2).mul(0.5).mean() #mean square error\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "        \n",
    "    \n",
    "    def train(self, env, gamma=1.0, num_episodes=100, policy_lr=None, value_lr=None, save_models=None):\n",
    "        if save_models: #list of episodes to save models\n",
    "                save_models.sort()\n",
    "        self.gamma = gamma\n",
    "        self._init_model(env, policy_lr, value_lr)\n",
    "\n",
    "        saved_models = {}\n",
    "        best_model = None\n",
    "\n",
    "        i = 0\n",
    "        episode_returns = np.zeros(num_episodes)\n",
    "        for episode in tqdm(range(num_episodes)):\n",
    "            state = env.reset()[0]\n",
    "            ep_return = 0\n",
    "            log_probs, rewards, values, entropies = [], [], [], []\n",
    "            for t in count():\n",
    "                i += 1\n",
    "                action, log_prob, entropy = self.policy_model.select_action(state) #select action and get corresponding log prob and dist entropy\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                \n",
    "                #gather log probs, rewards, and entropies to calculate policy gradient\n",
    "                log_probs.append(log_prob)\n",
    "                rewards.append(reward)\n",
    "                entropies.append(entropy)\n",
    "                #get estimated value of current state\n",
    "                values.append(self.value_model(state))\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                ep_return += reward * gamma**t #add discounted reward to return\n",
    "                if terminated or truncated:\n",
    "                    #save best model\n",
    "                    if ep_return >= episode_returns.max():\n",
    "                        copy = self.policy_model_fn(len(env.observation_space.sample()), env.action_space.n)\n",
    "                        copy.load_state_dict(self.policy_model.state_dict())\n",
    "                        best_model = copy\n",
    "                    #copy and save model\n",
    "                    if save_models and len(saved_models) < len(save_models) and episode+1 == save_models[len(saved_models)]:\n",
    "                        copy = self.policy_model_fn(len(env.observation_space.sample()), env.action_space.n)\n",
    "                        copy.load_state_dict(self.policy_model.state_dict())\n",
    "                        saved_models[episode+1] = copy\n",
    "\n",
    "                    episode_returns[episode] = ep_return\n",
    "                    break\n",
    "            #apply gradient optimization at end of each episode\n",
    "            self._optimize_model(rewards, values, log_probs, entropies)\n",
    "        \n",
    "        return episode_returns, best_model, saved_models"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
