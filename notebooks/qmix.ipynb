{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QMIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Original paper (compact)](https://arxiv.org/pdf/1803.11485.pdf)\n",
    "\n",
    "[Original paper (extended)](https://arxiv.org/pdf/2003.08839.pdf)\n",
    "\n",
    "[Original implementation](https://github.com/oxwhirl/pymarl)\n",
    "\n",
    "- **Only suitable in cooperative environments**\n",
    "- Value-based multi-agent deep RL algorithm, with DQN foundation\n",
    "  - Learns value ($Q$) function using experiences sampled from a replay memory\n",
    "  - Stabilizes target values with target network/parameters\n",
    "- Each agent $a$ has its own action-value function $Q_a(\\tau^a, u^a)$\n",
    "  - $Q_a$ is represented as a *deep recurrent Q-network* (DRQN), with the use of a gated recurrent unit (GRU)\n",
    "  - Individual agents can benefit from using their entire action-observation history ($\\tau^a$) in partially observable settings\n",
    "  - $u^a$ is the action to take in the current state\n",
    "- QMIX performs centralized training by learning a joint action-value function $Q_\\text{tot}(\\boldsymbol{\\tau},\\textbf{u})$ where $\\boldsymbol{\\tau}$ is a joint action-observation history and $\\textbf{u}$ is a joint action\n",
    "  - $Q_\\text{tot}$ is composed of $Q_a$ all agents $a$ in a way such that: $$\\underset{\\textbf{u}}{\\mathrm{argmax}}\\;Q_\\text{tot}(\\boldsymbol{\\tau},\\textbf{u}) = \\left\\{\\underset{\\textbf{u}^a}{\\mathrm{argmax}}\\;Q_a(\\tau^a,\\text{u}^a)\\;|\\; \\forall a\\in A\\right\\}$$ allowing for decentralized execution\n",
    "  - The above condition holds as long as $Q_\\text{tot}$ is monotonic (increasing) with respect to each $Q_a$: $$\\frac{\\partial Q_\\text{tot}}{\\partial Q_a}\\ge 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "- In practice, $Q_\\text{tot}$ is the output of a *mixing network* that combines the outputs of each *agent network*, $Q_a$\n",
    "  - The weights and biases of the mixing network are produced by hypernetworks that take in total state information and also train off of replay memory\n",
    "  - To ensure monotonicty, the weights produced are set as non-negative via absolute value activation\n",
    "\n",
    "[Network implementation details here](https://arxiv.org/pdf/2003.08839.pdf#page=13)\n",
    "\n",
    "![qmix network](<qmix network.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "- $\\tau^a$ - individual agent's observation history\n",
    "- $u^a$ - action chosen by individual agent\n",
    "- $\\textbf{u}$ - joint action (actions of all agents)\n",
    "\n",
    "![qmix](qmix.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
