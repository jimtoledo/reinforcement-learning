{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep deterministic policy gradient (DDPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Deterministic policy gradient paper](https://proceedings.mlr.press/v32/silver14.pdf)\n",
    "\n",
    "[Deep deterministic policy gradient paper](https://arxiv.org/pdf/1509.02971.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Only applicable to *continuous* action spaces**\n",
    "- In continuous action spaces, greedy policy improvement using $\\underset{a'}{\\mathrm{argmax}}\\;Q(s',a'; \\theta)$, becomes very problematic as it requires a global maximisation at every step\n",
    "- Alternatively, we can use a deterministic policy function to approximate the best action and move the policy in the direction of the gradient of $Q$:\n",
    "  - The critic, $Q(s,a;\\theta^Q)$ is learned using the Bellman equation like in Q-learning/DQN\n",
    "  - The actor, $\\mu(s;\\theta^\\mu)$ is updated by applying the chain rule to the gradient of $Q$. This is derived from the *Deterministic Policy Gradient Theorem*, [proven by Silver et al.](https://proceedings.mlr.press/v32/silver14.pdf):\n",
    "    - $$\\nabla_{\\theta^\\mu}\\approx \\mathbb{E}_{s\\sim \\rho^\\beta}\\left[\\nabla_{\\theta^\\mu} Q(s,\\mu(s;\\theta^\\mu);\\theta^Q)\\right]$$\n",
    "    - $$=\\mathbb{E}_{s\\sim \\rho^\\beta}\\left[\\nabla_{a}Q(s,a|\\theta^Q) \\nabla_{\\theta^\\mu}\\mu(s;\\theta^\\mu)\\right]$$\n",
    "- DDPG uses many of the same stabilizing techniques as in DQN\n",
    "  - Experience replay\n",
    "  - Target networks (for both actor and critic)\n",
    "- DDPG typically learns off-policy, with an exploration policy $u'$ that adds noise sampled from a noise process $\\cal{N}$\n",
    "  - $$\\mu'(s_t)=\\mu(s_t|\\theta_t^\\mu) + \\cal{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ddpg](ddpg.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
