{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Code implementations derived from https://github.com/mimoralea/gdrl and https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For simple environments, the value function is represented with a simple lookup table\n",
    "- For environments with very large and/or continuous state and action spaces, tables may not be feasible\n",
    "- In these cases, we must approximate the value function with a parameterized ($\\theta$) function:\n",
    "  - $\\hat{v}(s ; \\theta) \\approx v_\\pi(s)$\n",
    "  - $\\hat{q}(s, a ; \\theta) \\approx q_\\pi(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Fitted Q (NFQ) Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Approximates action-value ($Q$) function using a neural network\n",
    "- A couple of considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value function to approximate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- State-value function $v(s)$\n",
    "- Action-value function $q(s,a)$\n",
    "- Action-advantage function $a(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input nodes\n",
    "- Output nodes\n",
    "- \\# and size of hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- State-action-in-value-out architecture\n",
    "  - State and action variable input nodes\n",
    "  - Value output (ex: $Q(s,a)$) as single node\n",
    "- State-in-values-out architecture\n",
    "  - State variable input nodes\n",
    "  - Vector of values for each action as output nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fully-connected neural network\n",
    "class FCQ(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 output_dim, \n",
    "                 hidden_dims=(32,32), #define hidden layers as tuple where each element is an int representing # of neurons at a layer\n",
    "                 activation_fc=nn.ReLU):\n",
    "        super(FCQ, self).__init__()\n",
    "\n",
    "        hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            hidden_layers.append(activation_fc())\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            activation_fc(),\n",
    "            *hidden_layers,\n",
    "            nn.Linear(hidden_dims[-1], output_dim)\n",
    "        )\n",
    "\n",
    "        device = \"cpu\"\n",
    "        #if torch.cuda.is_available():\n",
    "        #    device = \"cuda\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def _format(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self._format(state)\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def numpy_float_to_device(self, variable):\n",
    "        variable = torch.from_numpy(variable).float().to(self.device)\n",
    "        return variable\n",
    "    \n",
    "    #load experience samples to device\n",
    "    def load(self, experiences):\n",
    "        states, actions, rewards, new_states, is_terminals = experiences\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).long().to(self.device)\n",
    "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "        return states, actions, rewards, new_states, is_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OldFCQ(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 output_dim, \n",
    "                 hidden_dims=(32,32), \n",
    "                 activation_fc=F.relu):\n",
    "        super(OldFCQ, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def _format(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self._format(state)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def numpy_float_to_device(self, variable):\n",
    "        variable = torch.from_numpy(variable).float().to(self.device)\n",
    "        return variable\n",
    "    \n",
    "    def load(self, experiences):\n",
    "        states, actions, new_states, rewards, is_terminals = experiences\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).long().to(self.device)\n",
    "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "        return states, actions, new_states, rewards, is_terminals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ideal objective would be to minimize loss with respect to the optimal action-value function $q^*$, with estimate $Q$\n",
    "  - $$L_i(\\theta_i) = \\mathbb{E}_{s,a}[(q^*(s,a) - Q(s,a;\\theta_i))^2]$$\n",
    "- Problem: we don't have $q^*$, nor do we have the optimal policy $\\pi^*$ to sample from $q^*$\n",
    "- Solution: Use principles of genralized policy iteration (GPI), alternating between policy evaluation and policy improvement to find good policies\n",
    "  - Convergence guarentees no longer exist for non-linear function approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targets for policy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Monte Carlo:** use all reward found in trajectory from start state to terminal state\n",
    "- **Temporal Difference (TD(0)):** use value of next state as an estimate of future rewards (Bellman equation)\n",
    "  - On-policy = SARSA\n",
    "  - Off-policy (approximate using greedy) = Q-learning\n",
    "- **N-step TD:** Like TD, but instead of bootstrapping after one step, use $n$ steps\n",
    "- **TD($\\lambda$):** Exponentially decaying n-step targets\n",
    "- ***Notes on TD targets:***\n",
    "  - Only backpropogate through the predicted values, not the target!\n",
    "    - $$\\nabla_{\\theta_i}L_i(\\theta_i) = \\mathbb{E}_{s,a,r,s'}[(y_i - Q(s,a;\\theta_i))\\nabla_{\\theta_i}Q(s,a;\\theta_i)]$$\n",
    "    - where $$y_i = r+\\gamma\\underset{a'}\\max\\;Q(s',a';\\theta_i)$$\n",
    "  - In supervised learning, the targets are constant *true* values\n",
    "  - In RL, the TD target is not a *true* value; it is the reward (constant) plus the discounted value of the next state (comes from the model)\n",
    "  - In forming the TD target, the predicted values of the next states need to be treated as a constant\n",
    "    - In PyTorch, use *detach* method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient descent\n",
    "  - Update parameters according to opposite direction of the gradient of the loss function\n",
    "  - Stable under the assumption that data is independent and identically distributed (IID) and targets are stationary\n",
    "  - Cannot guarentee those assumptions in RL\n",
    "- Batch gradient descent\n",
    "  - Uses the entire dataset to calculate loss and gradient\n",
    "  - Steps towards gradient slowly at each step\n",
    "  - Can be too slow to be practical\n",
    "  - In RL, we do not have access to entire datasets in advance\n",
    "- Mini-batch gradient descent\n",
    "  - Uses fraction of data at a time (uniformly sampled mini-batch)\n",
    "  - Noisier updates, but faster processing of data\n",
    "  - ***Momentum*** variant updates weights using moving average of gradients instead of gradient at each step\n",
    "- Stochastic gradient descent\n",
    "  - Mini-batch gradient descent with batch size = 1\n",
    "  - Calculate and move towards gradient of single sample at each step\n",
    "- Root mean square propogation (RMSprop)\n",
    "  - Scales gradient in proportion to average magnitude of recent gradients (square root of the moving average of the square of gradients)\n",
    "- Adaptive momentum estimation (Adam)\n",
    "  - Combination of RMSprop and momentum\n",
    "  - Step in direction of moving average of gradients (momentum)\n",
    "  - Step in proportion to average magnitude of recent gradients (RMSprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overall concerns to address in RL:**\n",
    "- Targets are not stationary because target values depend on the values for the next state, which we estimate using the neural network\n",
    "- Samples are not IID when collected and processed online\n",
    "  - Most of the samples in a mini-batch will come from the same trajectory and policy\n",
    "  - Samples in mini-batches are internally correlated, but likely different from other mini-batches (different policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Separate network that is fixed for multiple steps\n",
    "  - Can also use *Polyak averaging* to smooth target network progression by mixing in a tiny bit of the online network at every step\n",
    "    - $\\theta_i^- = \\tau\\theta_i + (1-\\tau)\\theta_i^-$\n",
    "- Used to calculate more stationary targets\n",
    "- Helps improve chances of convergence, substantially reduces chance of divergence\n",
    "- Target calculation with target network weights $\\theta^-$: $$y_i = r+\\gamma\\underset{a'}\\max\\;Q(s',a';\\theta^-)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A replay buffer/memory holds experience samples for **several** steps, over many episodes (if environment terminates)\n",
    "- Mini-batches can then be uniformly sampled from a broad set of past experiences\n",
    "- Updates to the network are better distributed and more representative of the true value function\n",
    "- Agent can sample and train on every time step with a lower risk of divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, \n",
    "                 max_size=10000):\n",
    "        #initialize fixed-length np arrays for each element of experience tuple (state, action, reward, next state, done)\n",
    "        self.ss_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
    "        self.as_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
    "        self.rs_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
    "        self.ps_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
    "        self.ds_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
    "\n",
    "        self.max_size = max_size\n",
    "        self._idx = 0 #where to add next sample\n",
    "        self.size = 0\n",
    "    \n",
    "    def store(self, sample): #store sample into replay memory\n",
    "        s, a, r, p, d = sample\n",
    "        self.ss_mem[self._idx] = s\n",
    "        self.as_mem[self._idx] = a\n",
    "        self.rs_mem[self._idx] = r\n",
    "        self.ps_mem[self._idx] = p\n",
    "        self.ds_mem[self._idx] = d\n",
    "        \n",
    "        self._idx += 1\n",
    "        self._idx = self._idx % self.max_size #replace older samples when at max capacity\n",
    "\n",
    "        self.size += 1\n",
    "        self.size = min(self.size, self.max_size)\n",
    "\n",
    "\n",
    "    def sample(self, batch_size=1): #return batch of samples selected uniform randomly\n",
    "\n",
    "        idxs = np.random.choice(\n",
    "            self.size, batch_size, replace=False)\n",
    "        \n",
    "        experiences = np.vstack(self.ss_mem[idxs]), np.vstack(self.as_mem[idxs]), np.vstack(self.rs_mem[idxs]), np.vstack(self.ps_mem[idxs]), np.vstack(self.ds_mem[idxs])\n",
    "        return experiences\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full DQN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGreedyExpStrategy():\n",
    "    def __init__(self, init_epsilon=1.0, min_epsilon=0.1, decay_steps=20000):\n",
    "        self.epsilon = init_epsilon\n",
    "        self.init_epsilon = init_epsilon\n",
    "        self.decay_steps = decay_steps\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilons = 0.01 / np.logspace(-2, 0, decay_steps, endpoint=False) - 0.01\n",
    "        self.epsilons = self.epsilons * (init_epsilon - min_epsilon) + min_epsilon\n",
    "        self.t = 0\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        self.exploratory_action_taken = False\n",
    "        with torch.no_grad():\n",
    "            if np.random.rand() > self.epsilon:\n",
    "                action = model(state).detach().max(1).indices.view(1, 1).item() #choose action with highest estimated value\n",
    "            else:\n",
    "                action = np.random.randint(model(state).shape[1]) #random action\n",
    "\n",
    "        self.epsilon = self.min_epsilon if self.t >= self.decay_steps else self.epsilons[self.t]\n",
    "        self.t += 1\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, \n",
    "                value_model_fn = lambda num_obs, nA: FCQ(num_obs, nA), #state vars, nA -> model\n",
    "                value_optimizer_fn = lambda params, lr : optim.RMSprop(params, lr), #model params, lr -> optimizer\n",
    "                value_optimizer_lr = 1e-4, #optimizer learning rate\n",
    "                loss_fn = nn.MSELoss(), #input, target -> loss\n",
    "                exploration_strategy = EGreedyExpStrategy(), #module with select_action function (model, state) -> action\n",
    "                memory_size = 10000 #replay memory capacity\n",
    "                ):\n",
    "        self.value_model_fn = value_model_fn\n",
    "        self.value_optimizer_fn = value_optimizer_fn\n",
    "        self.value_optimizer_lr = value_optimizer_lr\n",
    "        self.loss_fn = loss_fn\n",
    "        self.exploration_strategy = exploration_strategy\n",
    "        self.memory = ReplayBuffer(memory_size)\n",
    "\n",
    "    def _init_model(self, env):\n",
    "        #initialize online and target models\n",
    "        self.online_model = self.value_model_fn(len(env.observation_space.sample()), env.action_space.n)\n",
    "        self.target_model = self.value_model_fn(len(env.observation_space.sample()), env.action_space.n)\n",
    "        self.target_model.load_state_dict(self.online_model.state_dict()) #copy online model parameters to target model\n",
    "        #initialize optimizer\n",
    "        self.optimizer = self.value_optimizer_fn(self.online_model.parameters(), lr=self.value_optimizer_lr)\n",
    "    \n",
    "    def _optimize_model(self):\n",
    "        experiences = self.memory.sample(self.batch_size)\n",
    "        experiences = self.online_model.load(experiences)\n",
    "        states, actions, rewards, next_states, is_terminals = experiences\n",
    "    \n",
    "        max_a_q_sp = self.target_model(next_states).detach().max(1)[0].unsqueeze(1) #values for next states\n",
    "        target_q_sa = rewards + (self.gamma * max_a_q_sp * (1 - is_terminals)) #calculate q target\n",
    "\n",
    "        q_sa = self.online_model(states).gather(1, actions) #get predicted q from model for each state, action pair\n",
    "\n",
    "        loss = self.loss_fn(q_sa, target_q_sa) #calculate loss between prediction and target\n",
    "\n",
    "        #optimize step (gradient descent)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def train(self, env, gamma=1.0, num_episodes=100, batch_size=64, n_warmup_batches = 5, tau=0.005, target_update_steps=1, save_models=None):\n",
    "        if save_models: #list of episodes to save models\n",
    "                save_models.sort()\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self._init_model(env)\n",
    "\n",
    "        saved_models = {}\n",
    "        best_model = None\n",
    "\n",
    "        i = 0\n",
    "        episode_returns = np.zeros(num_episodes)\n",
    "        for episode in tqdm(range(num_episodes)):\n",
    "            state = env.reset()[0]\n",
    "            ep_return = 0\n",
    "            for t in count():\n",
    "                i += 1\n",
    "                action = self.exploration_strategy.select_action(self.online_model, state) #use online model to select action\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                self.memory.store((state, action, reward, next_state, terminated)) #store experience in replay memory\n",
    "                \n",
    "                state = next_state\n",
    "\n",
    "                if len(self.memory) >= batch_size*n_warmup_batches: #optimize policy model\n",
    "                    self._optimize_model()\n",
    "\n",
    "                #update target network with tau\n",
    "                if i % target_update_steps == 0:\n",
    "                    #self.target_model.load_state_dict(self.online_model.state_dict())\n",
    "                    for target, online in zip(self.target_model.parameters(), self.online_model.parameters()):\n",
    "                        target_weights = tau*online.data + (1-tau)*target.data\n",
    "                        target.data.copy_(target_weights)\n",
    "\n",
    "                ep_return += reward * gamma**t #add discounted reward to return\n",
    "                if terminated or truncated:\n",
    "                    #save best model\n",
    "                    if ep_return >= episode_returns.max():\n",
    "                        copy = self.value_model_fn(len(env.observation_space.sample()), env.action_space.n)\n",
    "                        copy.load_state_dict(self.online_model.state_dict())\n",
    "                        best_model = copy\n",
    "                    #copy and save model\n",
    "                    if save_models and len(saved_models) < len(save_models) and episode+1 == save_models[len(saved_models)]:\n",
    "                        copy = self.value_model_fn(len(env.observation_space.sample()), env.action_space.n)\n",
    "                        copy.load_state_dict(self.online_model.state_dict())\n",
    "                        saved_models[episode+1] = copy\n",
    "\n",
    "                    episode_returns[episode] = ep_return\n",
    "                    break\n",
    "        \n",
    "        return episode_returns, best_model, saved_models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Traditional DQN often overestimates action-value functions due to *maximization bias*\n",
    "  - Performance suffers from positive bias as estimated values are often off-center\n",
    "- Double DQN is a practical improvement that does not drastically increase overhead\n",
    "- Select best action of next state using online network, but still use target network for target value estimates\n",
    "  - DQN target: $$y_i = r+\\gamma\\underset{a'}\\max\\;Q(s',a';\\theta^-)$$ $$y_i = r+\\gamma Q(s',\\underset{a'}{\\mathrm{argmax}}\\;Q(s',a';\\theta^-);\\theta^-)$$\n",
    "  - DDQN target: $$y_i = r+\\gamma Q(s',\\underset{a'}{\\mathrm{argmax}}\\;Q(s',a';\\theta_i);\\theta^-)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DDQN implements one small change to DQN\n",
    "class DDQN(DQN):\n",
    "    #only need to change the optimize model method, everything else is inherited from DQN\n",
    "    def _optimize_model(self):\n",
    "        experiences = self.memory.sample(self.batch_size)\n",
    "        experiences = self.online_model.load(experiences)\n",
    "        states, actions, rewards, next_states, is_terminals = experiences\n",
    "    \n",
    "        argmax_a_q_sp = self.online_model(next_states).max(1)[1] #select best action of next state according to online model\n",
    "        max_a_q_sp = self.target_model(next_states).detach()[np.arange(self.batch_size), argmax_a_q_sp].unsqueeze(1) #get values of next states using target network\n",
    "        target_q_sa = rewards + (self.gamma * max_a_q_sp * (1 - is_terminals)) #calculate q target\n",
    "\n",
    "        q_sa = self.online_model(states).gather(1, actions) #get predicted q from model for each state, action pair\n",
    "\n",
    "        loss = self.loss_fn(q_sa, target_q_sa) #calculate loss between prediction and target\n",
    "\n",
    "        #optimize step (gradient descent)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dueling DDQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The action advantage function is the difference in expected return when selecting action $a$ at state $s$ over following policy $\\pi$: $a_\\pi(s,a)=q_\\pi(s,a)-v_\\pi(s)$\n",
    "- We can rearrange this formula to see how the state-value function $v$ relates to the action-value function $q$: $q_\\pi(s,a)=v_\\pi(s,a)+a_\\pi(s)$\n",
    "- We can decompose a $Q$ function into two components:\n",
    "  - one that is independent on the action: $V(s)$\n",
    "  - one that is dependent on the action $A(s,a)$\n",
    "- Network architecture can be made to take advantage of this decomposition\n",
    "  - Network splits into two streams: a stream for the state-value function $V(s)$, and another for the action-advantage function $A(s,a)$\n",
    "  - $V(s)$ is represented as a single node; $A(s,a)$ should have a node for each action in the action space\n",
    "  - Final output layer should represent $Q(s,a)$ with a node for each action\n",
    "    - All output nodes are connected to $V(s)$ node\n",
    "    - For each action, output node connects to its corresponding $A(s,a)$ node\n",
    "- The action-value function, $Q(s,a;\\theta,\\alpha,\\beta) = V(s;\\theta,\\beta) + A(s,a;\\theta,\\alpha)$ is now parameterized by three sets of weights\n",
    "  - $\\theta$ - weights of the shared layers\n",
    "  - $\\alpha$ - weights of the action-advantage function stream\n",
    "  - $\\beta$ - weights of the state-value function stream\n",
    "- **Problem:** hard to discriminate between $V$ and $A$ from $Q$ using simple $Q=V+A$ function, in practice we subtract the mean of the advantages:\n",
    "  - $$Q(s,a;\\theta,\\alpha,\\beta) = V(s;\\theta,\\beta) + \\left(A(s,a;\\theta,\\alpha)-\\frac{1}{|A|}\\sum_{a'}A(s,a';\\theta,\\alpha)\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCDuelingQ(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 output_dim, \n",
    "                 hidden_dims=(32,32),\n",
    "                 v_hidden_dims = (32,),\n",
    "                 a_hidden_dims = (32,),\n",
    "                 activation_fc=nn.ReLU):\n",
    "        super(FCDuelingQ, self).__init__()\n",
    "        \n",
    "        #build hidden layers for features, value stream, and advantage stream\n",
    "        feature_hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            feature_hidden_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            feature_hidden_layers.append(activation_fc())\n",
    "\n",
    "        v_hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(v_hidden_dims)-1):\n",
    "            v_hidden_layers.append(nn.Linear(v_hidden_dims[i], v_hidden_dims[i+1]))\n",
    "            v_hidden_layers.append(activation_fc())\n",
    "\n",
    "        a_hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(a_hidden_dims)-1):\n",
    "            a_hidden_layers.append(nn.Linear(a_hidden_dims[i], a_hidden_dims[i+1]))\n",
    "            a_hidden_layers.append(activation_fc())\n",
    "\n",
    "        #build features, value stream, and advantage stream\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            activation_fc(),\n",
    "            *feature_hidden_layers\n",
    "        )\n",
    "\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[-1], v_hidden_dims[0]),\n",
    "            activation_fc(),\n",
    "            *v_hidden_layers,\n",
    "            nn.Linear(v_hidden_dims[-1], 1)\n",
    "        )\n",
    "\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[-1], a_hidden_dims[0]),\n",
    "            activation_fc(),\n",
    "            *a_hidden_layers,\n",
    "            nn.Linear(a_hidden_dims[-1], output_dim)\n",
    "        )\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def _format(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)      \n",
    "        return x\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self._format(state)\n",
    "        features = self.features(x)\n",
    "        values = self.value_stream(features)\n",
    "        advantages = self.advantage_stream(features)\n",
    "        qs = values + (advantages - advantages.mean())\n",
    "        return qs\n",
    "    \n",
    "    def numpy_float_to_device(self, variable):\n",
    "        variable = torch.from_numpy(variable).float().to(self.device)\n",
    "        return variable\n",
    "\n",
    "    def load(self, experiences):\n",
    "        states, actions, new_states, rewards, is_terminals = experiences\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).long().to(self.device)\n",
    "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "        return states, actions, new_states, rewards, is_terminals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(env, model, gamma=1, num_episodes=1):\n",
    "    ep_returns = []\n",
    "    for _ in range(num_episodes):\n",
    "        ep_return = 0\n",
    "        state = env.reset()[0]\n",
    "        for t in count():\n",
    "            action = model(state).detach().max(1).indices.view(1, 1).item()\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            ep_return += reward * gamma**t\n",
    "            state = next_state\n",
    "            if terminated or truncated:\n",
    "                ep_returns.append(ep_return)\n",
    "                break\n",
    "    return ep_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(dqn, env, **kwargs):\n",
    "    return dqn.train(env, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(value_model_fn = lambda num_obs, nA: FCQ(num_obs, nA, hidden_dims=(512, 128,)),\n",
    "          value_optimizer_lr = 0.0001,\n",
    "          exploration_strategy=EGreedyExpStrategy(min_epsilon=0.01),\n",
    "          memory_size=50000,\n",
    "          )\n",
    "\n",
    "ddqn = DDQN(value_model_fn = lambda num_obs, nA: FCQ(num_obs, nA, hidden_dims=(512, 128,)),\n",
    "          value_optimizer_lr = 0.0001,\n",
    "          exploration_strategy=EGreedyExpStrategy(min_epsilon=0.01),\n",
    "          memory_size=50000,\n",
    "          )\n",
    "\n",
    "dueling_ddqn = DDQN(value_model_fn = lambda num_obs, nA: FCDuelingQ(num_obs, nA, hidden_dims=(512,)),\n",
    "          value_optimizer_lr = 0.0001,\n",
    "          exploration_strategy=EGreedyExpStrategy(min_epsilon=0.01),\n",
    "          memory_size=50000,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqns = {'dqn' : dqn, 'ddqn' : ddqn, 'dueling_ddqn' : dueling_ddqn}\n",
    "dqn_results = {}\n",
    "for name, method in dqns.items():\n",
    "    print('Testing ' + name +'...')\n",
    "    dqn_results[name] = train_dqn(method, num_episodes=800, tau=0.01, batch_size=128, save_models=[1, 100, 250, 500, 800])\n",
    "\n",
    "print('Saving results...')\n",
    "import pickle\n",
    "with open('dqn.tests', 'wb') as file:\n",
    "    pickle.dump(dqn_results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "res = {}\n",
    "with open('dqn.tests', 'rb') as file:\n",
    "    res = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Total return at each episode')\n",
    "for test in res:\n",
    "    episode_returns = res[test][0]\n",
    "    plt.plot(range(len(episode_returns)), episode_returns, label=test)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Moving average of total return (k=100)')\n",
    "for test in res:\n",
    "    episode_returns = res[test][0]\n",
    "    plt.plot(range(len(episode_returns)), [np.mean(episode_returns[max(0,x-100):x+1]) for x in range(len(episode_returns))], label=test)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "pygame.init()\n",
    "font = pygame.font.SysFont('Comic Sans MS', 16)\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "env.metadata['render_fps']=0\n",
    "STEPS = 1000\n",
    "env.reset()[0]\n",
    "for method in res:\n",
    "    text = font.render(method, False, (0, 0, 0))\n",
    "    for i, model in res[method][2].items():\n",
    "        state = env.reset()[0]\n",
    "        ep_return = 0\n",
    "        return_sum = 0\n",
    "        episode = 1\n",
    "        for x in range(STEPS):\n",
    "            action = model(state).detach().max(1).indices.view(1, 1).item()\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            ep_return += reward\n",
    "            if terminated or truncated:\n",
    "                state = env.reset()[0]\n",
    "                return_sum += ep_return\n",
    "                ep_return = 0\n",
    "                episode += 1\n",
    "            else:\n",
    "                state = next_state\n",
    "            \n",
    "            text = font.render(f'{method} model at episode {i}', False, (0, 0, 0))\n",
    "            progress_text = font.render(f'{x}/{STEPS}', False, (0, 0, 0))\n",
    "            return_text = font.render(f'Current return at episode {episode}: {ep_return}', False, (0, 0, 0))\n",
    "            env.screen.blit(text, (10, 10))\n",
    "            env.screen.blit(progress_text, (10, 42))\n",
    "            env.screen.blit(return_text, (10, 74))\n",
    "            if return_sum:\n",
    "                mean_return_text = font.render(f'Mean return: {np.round(return_sum/(episode-1),3)}', False, (0, 0, 0))\n",
    "                env.screen.blit(mean_return_text, (10, 106))\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "            env.clock.tick(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dueling_ddqn = DDQN(value_model_fn = lambda num_obs, nA: FCDuelingQ(num_obs, nA, hidden_dims=(512,)),\n",
    "          value_optimizer_lr = 0.0001,\n",
    "          exploration_strategy=EGreedyExpStrategy(min_epsilon=0.01),\n",
    "          memory_size=50000,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, save_models = train_dqn(dueling_ddqn, num_episodes=500, tau=0.01, batch_size=128, save_models=[1, 10, 50, 100, 250, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dqdn.models', 'wb') as file:\n",
    "    pickle.dump(save_models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_models = []\n",
    "with open('dqdn.models', 'rb') as file:\n",
    "    save_models = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "pygame.init()\n",
    "font = pygame.font.SysFont('Comic Sans MS', 16)\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "env.metadata['render_fps']=120\n",
    "STEPS = 500\n",
    "env.reset()[0]\n",
    "for i, model in save_models.items():\n",
    "    state = env.reset()[0]\n",
    "    ep_return = 0\n",
    "    return_sum = 0\n",
    "    episode = 1\n",
    "    for x in range(STEPS):\n",
    "        action = model(state).detach().max(1).indices.view(1, 1).item()\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        ep_return += reward\n",
    "        if terminated or truncated:\n",
    "            state = env.reset()[0]\n",
    "            return_sum += ep_return\n",
    "            ep_return = 0\n",
    "            episode += 1\n",
    "        else:\n",
    "            state = next_state\n",
    "        \n",
    "        text = font.render(f'Dueling DDQN model at episode {i}', False, (0, 0, 0))\n",
    "        progress_text = font.render(f'{x}/{STEPS}', False, (0, 0, 0))\n",
    "        return_text = font.render(f'Current return at episode {episode}: {ep_return}', False, (0, 0, 0))\n",
    "        env.screen.blit(text, (10, 10))\n",
    "        env.screen.blit(progress_text, (10, 42))\n",
    "        env.screen.blit(return_text, (10, 74))\n",
    "        if return_sum:\n",
    "            mean_return_text = font.render(f'Mean return: {np.round(return_sum/(episode-1),3)}', False, (0, 0, 0))\n",
    "            env.screen.blit(mean_return_text, (10, 106))\n",
    "        pygame.event.pump()\n",
    "        pygame.display.update()\n",
    "        env.clock.tick(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn_1_cartpole-episode-0.mp4.\n",
      "Moviepy - Writing video c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn_1_cartpole-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn_1_cartpole-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn_10_cartpole-episode-0.mp4.\n",
      "Moviepy - Writing video c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn_10_cartpole-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn_10_cartpole-episode-0.mp4\n",
      "Moviepy - Building video c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn_50_cartpole-episode-0.mp4.\n",
      "Moviepy - Writing video c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn_50_cartpole-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn_50_cartpole-episode-0.mp4\n",
      "Moviepy - Building video c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn_100_cartpole-episode-0.mp4.\n",
      "Moviepy - Writing video c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn_100_cartpole-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn_100_cartpole-episode-0.mp4\n",
      "Moviepy - Building video c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn_250_cartpole-episode-0.mp4.\n",
      "Moviepy - Writing video c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn_250_cartpole-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn_250_cartpole-episode-0.mp4\n",
      "Moviepy - Building video c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn_500_cartpole-episode-0.mp4.\n",
      "Moviepy - Writing video c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn_500_cartpole-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn_500_cartpole-episode-0.mp4\n"
     ]
    }
   ],
   "source": [
    "for i, model in save_models.items():\n",
    "    env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "    env.metadata['render_fps']=30\n",
    "    env = gym.wrappers.RecordVideo(env, video_folder='recordings/ddqn', episode_trigger = lambda x: x==0, name_prefix=f'ddqn_{i}_cartpole', disable_logger=False)\n",
    "    state = env.reset()[0]\n",
    "    while True:\n",
    "        action = model(state).detach().max(1).indices.view(1, 1).item()\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dueling_ddqn = DDQN(value_model_fn = lambda num_obs, nA: FCDuelingQ(num_obs, nA, hidden_dims=(512,)),\n",
    "          value_optimizer_lr = 0.0001,\n",
    "          exploration_strategy=EGreedyExpStrategy(min_epsilon=0.01),\n",
    "          memory_size=50000,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1603347702117002\\.conda\\envs\\rl_env\\Lib\\site-packages\\gymnasium\\wrappers\\record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn\\ddqn_cartpole-episode-0.mp4.\n",
      "Moviepy - Writing video c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn\\ddqn_cartpole-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/500 [00:01<12:22,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn\\ddqn_cartpole-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 100/500 [04:13<23:10,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn\\ddqn_cartpole-episode-100.mp4.\n",
      "Moviepy - Writing video c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn\\ddqn_cartpole-episode-100.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 101/500 [04:14<28:16,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn\\ddqn_cartpole-episode-100.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 200/500 [09:27<16:08,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn\\ddqn_cartpole-episode-200.mp4.\n",
      "Moviepy - Writing video c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn\\ddqn_cartpole-episode-200.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 201/500 [09:27<19:21,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn\\ddqn_cartpole-episode-200.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 300/500 [13:09<00:23,  8.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn\\ddqn_cartpole-episode-300.mp4.\n",
      "Moviepy - Writing video c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn\\ddqn_cartpole-episode-300.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 302/500 [13:10<00:35,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn\\ddqn_cartpole-episode-300.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 400/500 [13:52<00:12,  8.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn\\ddqn_cartpole-episode-400.mp4.\n",
      "Moviepy - Writing video c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn\\ddqn_cartpole-episode-400.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 402/500 [13:52<00:19,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\1603347702117002\\Documents\\MARL\\reinforcement-learning\\recordings\\ddqn\\ddqn_cartpole-episode-400.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [14:04<00:00,  1.69s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 99., 134.,  68.,  99.,  74., 170.,  39.,  70., 124., 174.,  83.,\n",
       "         56.,  73., 138.,  46.,  50.,  58., 132.,  65.,  41.,  91., 113.,\n",
       "        186., 106., 232.,  78.,  95.,  75.,  96.,  85., 171., 120., 114.,\n",
       "        164., 137., 203., 145., 146., 180., 273., 215., 263., 162., 285.,\n",
       "        313., 405., 500., 487., 456., 393., 409., 489., 319., 500., 500.,\n",
       "        500., 500., 500., 344., 500., 500., 318., 388., 500., 277., 500.,\n",
       "        500., 412., 361., 500., 500., 500., 500., 496., 500., 500., 370.,\n",
       "        374., 500., 486., 500., 500., 384., 472., 500., 490., 500., 500.,\n",
       "        500., 309., 303., 500., 351., 500., 406., 309., 405., 354., 424.,\n",
       "        500., 454., 496., 490., 500., 462., 317., 345., 380., 327., 347.,\n",
       "        500., 287., 301., 315., 340., 327., 365., 392., 275., 366., 345.,\n",
       "        312., 270., 288., 296., 259., 309., 278., 311., 274., 309., 303.,\n",
       "        296., 279., 279., 328., 389., 265., 274., 293., 311., 349., 286.,\n",
       "        290., 279., 258., 323., 282., 298., 257., 310., 296., 272., 277.,\n",
       "        296., 291., 287., 253., 271., 241., 252., 244., 240., 250., 279.,\n",
       "        254., 266., 241., 279., 258., 284., 288., 286., 285., 284., 270.,\n",
       "        294., 291., 311., 278., 304., 275., 276., 287., 309., 271., 259.,\n",
       "        271., 281., 331., 314., 320., 305., 313., 325., 328., 300., 308.,\n",
       "        341., 294., 315., 335., 292., 309., 313., 328., 326., 300., 321.,\n",
       "        423., 388., 411., 367., 362., 446., 500., 299., 458., 480., 500.,\n",
       "        316., 373., 500., 500., 443., 500., 373., 299., 443., 479., 496.,\n",
       "        349., 500., 395., 345., 446., 321., 264., 268., 253., 246., 249.,\n",
       "        244., 238., 224., 214., 211., 195., 202., 204., 185., 190., 179.,\n",
       "        169., 170., 166., 162., 154., 158., 145., 139., 143., 136., 134.,\n",
       "        134., 126., 129., 120., 123.,  18.,  17.,  11.,  14.,  15.,  15.,\n",
       "        120.,  10., 123.,  16.,  11.,  12.,  11.,  15.,  16.,  16.,  15.,\n",
       "         13.,  11.,  10.,  10.,  11.,  14.,  11.,  11.,  13.,  12.,  12.,\n",
       "         12.,  12.,  11.,   9.,  13.,  12.,  10.,   9.,  12.,  10.,  12.,\n",
       "         11.,  12.,  13.,  10.,  12.,   9.,  10.,   9.,  10.,  11.,  10.,\n",
       "         11.,  10.,  11.,  13.,  10.,  11.,  10.,  12.,  12.,   9.,  12.,\n",
       "         10.,  10.,  11.,  10.,  11.,  11.,  13.,  12.,  12.,  12.,  10.,\n",
       "         13.,  12.,  11.,  15.,  10.,  12.,  10.,  12.,  10.,  13.,  12.,\n",
       "         11.,  15.,  11.,  11.,  13.,  11.,  12.,  14.,  15.,  11., 117.,\n",
       "        123., 135., 149., 165., 194., 291., 500., 223., 191., 184., 173.,\n",
       "        168., 157., 146.,  15., 138.,  13.,  13.,  16.,  17.,  14., 119.,\n",
       "         11.,  12.,  12.,  12.,  13.,  13.,  13.,  10.,  13.,  11.,  11.,\n",
       "         12.,  12.,  13.,  11.,  12.,  13.,  10.,  10.,  12.,  12.,  10.,\n",
       "         12.,  11.,  11.,  11.,  10.,  11.,  12.,  10.,  11.,  11.,  12.,\n",
       "         11.,  11.,  12.,  11.,  10.,  10.,  11.,  11.,  10.,  12.,  12.,\n",
       "         11.,  11.,  10.,  12.,  11.,  11.,  11.,  10.,  12.,  10.,  11.,\n",
       "         12.,   9.,  13.,  12.,  11.,  12.,  12.,  10.,  11.,  10.,  12.,\n",
       "         12.,  11.,  10.,  12.,  11.,  12.,  11.,  12.,  11.,  11.,  11.,\n",
       "         12.,  10.,  11.,  12.,  11.,  13.,  13.,  13.,  13.,  12.,  12.,\n",
       "         13.,  13.,  12.,  14.,  12.,  12.,  13.,  13.,  13.,  12.,  13.,\n",
       "         13.,  12.,  13.,  12.,  13.,  14.,  12.,  15.,  14.,  15.,  12.,\n",
       "         14.,  12.,  15.,  16.,  17.]),\n",
       " FCDuelingQ(\n",
       "   (features): Sequential(\n",
       "     (0): Linear(in_features=4, out_features=512, bias=True)\n",
       "     (1): ReLU()\n",
       "   )\n",
       "   (value_stream): Sequential(\n",
       "     (0): Linear(in_features=512, out_features=32, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "   )\n",
       "   (advantage_stream): Sequential(\n",
       "     (0): Linear(in_features=512, out_features=32, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=32, out_features=2, bias=True)\n",
       "   )\n",
       " ),\n",
       " {})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "env.metadata['render_fps']=60\n",
    "env = gym.wrappers.RecordVideo(env, video_folder='recordings/ddqn', episode_trigger = lambda x: x % 100 == 0, name_prefix=f'ddqn_cartpole', disable_logger=False)\n",
    "train_dqn(dueling_ddqn, env, num_episodes=500, tau=0.01, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
